# 数据结构与算法

《算法导论》很全面和重理论，里面有非常复杂的数学证明和推理。。   要注重真实的开发场景。

边读边练，学习数据结构与算法读多了，在写代码的时候，会不自觉的考虑性能方面的问题。写时间复杂度和空间复杂度高的垃圾代码的情况越少。

- 开源项目的源码中会大量使用数据结构与算法的知识处理性能方面的问题

- 工作求职需要
- 扎实的基础能很快的帮助学习新的技术和知识

关注代码层面的编程能力和计算机基础知识。

- 关注那些实际开发中非常实用的数据结构与算法
- 知道怎么用，为什么需要用这种数据结构和这种算法，理解背后的设计思想
- 理解每种数据结构与算法的真实使用场景



第一部分：

- 时间和空间复杂度分析
- 大O表示法
- 各种复杂度的分析技巧
- 最好，最坏，平均，均摊复杂度分析方法



第二部分：

- 基础和常用的数据结构与算法
- 每种数据结构与算法的软件开发实例和优化
- 各种数据解构与算法的技巧



第三部分：

- 不常用的数据结构与算法



第四部分：

- 根据开源项目，框架和系统设计，讲解他们背后的数据结构与算法（实战）





为什么学习？

- 大厂面试必考
- 对于大部分业务，更多的是利用已经封装好的现成的接口、类库来堆砌、翻译业务逻辑，很少需要自己实现数据结构和算法。如果不知道它们背后的原理，用到的算法与数据结构，就难以用好、用对它们
- 各种框架、中间件和底层系统都揉和了很多基础数据结构和算法的设计思想，所以学习数据结构与算法能帮助理解框架源码
- 帮助编写开源项目的代码质量
- 面对海量数据时，性能几乎时必须考虑的问题，因为数据量一大，不同的数据结构和算法能产生数量级层次的性能差异
- 建立时间复杂度，空间复杂度意识



---

**一定要动手写**

**一定要动手写**

**一定要动手写**

---



针对学了忘，忘了学的情况，可能是没掌握学习的方法和重点，走马观花。

不能灵活应用，可能是没有优秀的教材教如何使用，同时知识点掌握不够牢固，要灵活应用是需要到一定的高度后的自然而然的事。

**边学边练，适度刷题**

**多问多思考，多看留言、总结**

**避免一知半解，最大限度消化每节的内容**

**在枯燥的学习重，给自己设立一个切合实际的可行目标**

**对于一开始学不好的东西，间隔性的反复学习和花时间酝酿**



## 学习指导

在分析某个算法的时间、空间复杂度的时候，常常用到数学知识。

先把最简单、最基础、最重要的知识点掌握好，再去研究难度较高、更加高级的知识点，这样由易到难、循序渐进的学习路径。



目标：

- 需要掌握到什么程度
- 具体如何来学习
- 学习某个算法或者数据结构的产生，特点，适合解决的问题和实际的应用场景



### 什么是数据结构

数据结构

定义：广义上，数据结构是指一组数据的存储结构，这些数据之间是有联系的。



### 什么是算法

算法

定义：广义上，操作数据的一组方法，每种数据结构都有更适合它的算法。



### 数据结构与算法的关系

数据结构与算法是相辅相成的，**数据结构是为算法服务的，算法要作用在特定的数据结构之上。**

比如，数组的特点是能快速实现随机访问，常用的二分查找算法就是依赖于数组，如果用的是链表，二分查找就无法工作的，因为链表是不支持随机访问的。

数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。







## 复杂度分析

数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行得更
快，如何让代码更省存储空间。

为此必须有一个考量的指标。所以评价一个算法的执行效率的两个维度：**时间复杂度和空间复杂度**。

时间复杂度：算法执行的时间多少。

空间复杂度：算法执行过程中占用的内存大小。



评估算法的方法—— **事后统计法**

**事后统计法**：把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。

但是这种方法有很大的局限性：

1. **测试结果非常依赖测试环境** 
2. **测试结果受数据规模的影响很大**

为此，需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。—— **大 O 复杂度表示法**



时间、空间复杂度分析方法—— **大 O 复杂度表示法**

粗略地讲，算法的执行效率就是算法代码执行的时间。

估算下列代码执行的时间：

```c
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum = sum + i;
  }
  return sum;
}
```

在CPU层面层面，代码的每一行都执行着类似的操作：**读数据-运算-写数据**。虽然每行代码对应的指令的条数和执行时间不同，但是，这里只是粗略估计，
所以可以假设每行代码执行的时间都一样，为 unit_time。在这个假设的基础之上，这段代码的总执行时间是多少呢？

第2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要2n\*unit_time 的执行时间，所以这段代码总的执行时间就是 (2n+2)\*unit_time。可以看出来，**所有代码的执行时间 T(n) 与每行代码的执行次数成正比。**



```c
int cal(int n) {
  int sum = 0;
  int i = 1;
  int j = 1;
  for (; i <= n; ++i) {
    j = 1;
    for (; j <= n; ++j) {
      sum = sum + i * j;
    }
  }
}
```

依旧假设每个语句的执行时间是 unit_time。那这段代码的总执行时间 T(n) 是多少呢？

第 2、3、4 行代码，每行都需要 1 个 unit_time 的执行时间，第 5、6 行代码循环执行了n 遍，需要 2n\*unit_time 的执行时间，第 7、8 行代码循环执行了 n 遍，所以需要 2n^2\* unit_time 的执行时间。所以，整段代码总的执行时间 T(n) = (2n^2 +2n+3)\*unit_time。

通过上面两段代码的分析得出：**所有代码的执行时间 T(n) 与每行代码的执行次数 n成正比。**

总结得出公式：

![image-20230228123832886](C:/Users/shuyi/Desktop/study-notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%9E%81%E5%AE%A2.images/image-20230228123832886.png)

T(n)：表示代码执行的时间

n：表示数据规模的大小

f(n)：表示每行代码执行的次数总和，因为这是一个公式，所以用 f(n)
来表示

 O：表示代码的执行时间 T(n) 与 f(n) 表达式成正比



第一段代码中的 T(n) = O(2n+2)

第二段代码中的 T(n) = O(2n^2 +2n+3)。

这就是大 O 时间复杂度表示法。大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示**代码执行时间随数据规模增长的变化趋势**，所以，也叫作**渐进时间复杂度**
（asymptotic time complexity），简称时间复杂度。

当n很大时，公式中的**低阶、常量、系数**三部分并不左右增长趋势，所以都可以忽略。只需要记录一个**最大量级**就可以了，如果用大 O表示法表示刚讲的那两段代码的时间复杂度，就可以记为：T(n) = O(n)； T(n) = O(n^2 )。



### 分析时间复杂度

分析时的注意点：

1. **只关注循环执行次数最多的一段代码**

   大O表示法只是表示一种变化趋势，通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级。所以，在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了。

2.  **加法法则：总复杂度等于量级最大的那段代码的复杂度**

   如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).

3.  **乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积**

   如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))。

   也就是说，假设 T1(n) = O(n)，T2(n) = O(n2)，则 T1(n) * T2(n) = O(n3)。落实到具体的代码上，可以把乘法法则看成是**嵌套循环**。



```c
int cal(int n) {
  int sum_1 = 0;
  int p = 1;
  for (; p < 100; ++p) {
    sum_1 = sum_1 + p;
  }

  int sum_2 = 0;
  int q = 1;
  for (; q < n; ++q) {
    sum_2 = sum_2 + q;
  }

  int sum_3 = 0;
  int i = 1;
  int j = 1;
  for (; i <= n; ++i) {
    j = 1; 
    for (; j <= n; ++j) {
      sum_3 = sum_3 +  i * j;
    }
  }

  return sum_1 + sum_2 + sum_3;
}
```

这个代码分为三部分，分别是求 sum_1、sum_2、sum_3。可以分别分析每一部分的时间复杂度，然后把它们放到一块儿，再取一个量级最大的作为整段代码的复杂度。

第一段代码循环执行了 100 次，所以是一个常量的执行时间，跟 n 的规模无关。

强调一下，即便这段代码循环 10000 次、100000 次，只要是一个已知的数，跟 n 无关，照样也是常量级的执行时间。当 n 无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到**时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势**，所以不管常量的执行时间多大，都可以忽略掉。因为它本身对**增长趋势**并没有影响。

那第二段代码和第三段代码的时间复杂度是多少呢？答案是 O(n) 和 O(n^2)。

综合这三段代码的时间复杂度，取其中最大的量级。所以，整段代码的时间复杂度就为 O(n^2)。也就是说：**总的时间复杂度就等于量级最大的那段代码的时间复杂度**。那我们将这个规律抽象成公式就是：

如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).



```c
int cal(int n) {
  int ret = 0; 
  int i = 1;
  for (; i < n; ++i) {
    ret = ret + f(i);
  } 
} 

int f(int n) {
  int sum = 0;
  int i = 1;
  for (; i < n; ++i) {
    sum = sum + i;
  } 
  return sum;
}
```

单独看 cal() 函数。假设 f() 只是一个普通的操作，那第 4～6 行的时间复杂度就是，T1(n) = O(n)。但 f() 函数本身不是一个简单的操作，它的时间复杂度是 T2(n) = O(n)，所以，整个 cal() 函数的时间复杂度就是，T(n) = T1(n) * T2(n) = O(n*n) = O(n2)。



### 常见时间复杂度实例

常见的复杂度量级：

![image-20230228130554479](C:/Users/shuyi/Desktop/study-notes/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E6%9E%81%E5%AE%A2.images/image-20230228130554479.png)

这些复杂度量级几乎涵盖了今后可以接触的所有代码的复杂度量级。

复杂度量级分类：**多项式量级和非多项式量级**。其中，非多项式量级只有两个：O(2n) 和 O(n!)。

当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。主要来看几种常见的**多项式时间复杂度**。



#### O(1)































- 必须掌握
- 简单代码能很快分析出时间、 空间复杂度
- 对于复杂点的代码掌握递推公式和递归树

目标：不看文章分析，能自行分析专栏中大部分数据结构和算法的时间、空 间复杂度



### 数组、栈、队列

- 作为基础的数据结构，数组、 栈、队列，是后续很多复杂数据结构和算法的基础
- 必须掌握

目标：能自己实现动态数组、栈、队列



### 链表

- 理论内容不多，但链表上的操作却很复杂
- 面试中经常会考察

目标：不只是能看懂专栏中的内容，还能将专栏中提到的经典链表题目，比如链表反转、求中间结点等，轻松无 bug 地实现出来。



### 递归

- 很多数据结构和算法的代 码实现，都要用到递归
- 递归相关的理论知识也不多，所以还是要多练
- 先在网上找些简单的题目练手，再慢慢过渡到更加有难度的

目标：轻松写出二叉树遍历、八皇后、背包问题、DFS 的递归代码





​	

